# Child Workflow for Process 8516
# Process ID : 8516
# Process Name - EDW_TRIPS: Item 26 File Export
# Calls a stored procedure - EDW_TRIPS: Item 26 File Export()


# labels
# workflow:child
# process_id:8516
# callout:sql

main:
    steps:
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - bucket_name: "BUCKET_NAME"
          - year: ${text.split(time.format(sys.now()),"-")[0]}
          - month: ${text.split(time.format(sys.now()),"-")[1]}
          - year_month: ${year+"_"+month}
          - table_name: ${"Item26_"+year_month}     #Extracts table name 
          - dataset_name: "Finance_reports_export"  #Extracts Dataset name

    - EDW_TRIPS_Item_26:
        try:
            call: googleapis.bigquery.v2.jobs.query
            args:
                projectId: ${project_id}
                body:
                    useLegacySql: false
                    useQueryCache: false 
                    query: ${"CALL FINANCE_REPORTS_EXPORT.Item26_Export('" +bucket_name+ "');"}
            result: query_result    
        except:
            as: e
            steps:
                - raise_query_error:
                    raise: ${e}   

    - Set_Job_Id:
        assign:
          - sp_job_id : ${query_result.jobReference.jobId}
          - location : ${query_result.jobReference.location}

    - Get_Job_Info:
        try:
            call: googleapis.bigquery.v2.jobs.get
            args:
                projectId: ${project_id}
                jobId: ${sp_job_id}
                location: ${location}
            result: job_info   #Storing result into variable job_info
        except:
            as: e
            steps:
                - raise_job_error:
                    raise: ${e}  

    - Check_Job_State:
        switch:
          - condition: ${job_info.status.state != "DONE"}
            steps: 
              - wait_30_s:
                  call: sys.sleep # Polling through sleep
                  args:
                    seconds: 30
                  next: Get_Job_Info

    - Get_Sp_Job_Result:
        try:
            call: googleapis.bigquery.v2.jobs.getQueryResults
            args:
                projectId: ${project_id}
                jobId: ${sp_job_id}
                location: ${location}
            result: sp_result
        except:
            as: e
            steps:
                - raise_sp_error:
                    raise: ${e}

    #Creates a Bigquery export job from BQ Table to Google Cloud Storage
    - EDW_TRIPS_Item_26_File_Export:
        try:
            call: googleapis.bigquery.v2.jobs.insert
            args:
                projectId: ${project_id}
                body:
                    configuration:
                      extract:
                        destinationFormat: CSV
                        destinationUris: 
                            - ${"gs://"+bucket_name+"/Exports/Item26/"+year_month+"/Item_26_"+year_month+".csv"}
                        sourceTable:
                            datasetId: ${dataset_name}
                            projectId: ${project_id}
                            tableId: ${table_name}
                connector_params:
                    timeout: 3600
            result: job_result    
        except:
            as: e
            steps:
                - raise_export_error:
                    raise: ${e}

    - returnOutput:
            return: '${job_result}'